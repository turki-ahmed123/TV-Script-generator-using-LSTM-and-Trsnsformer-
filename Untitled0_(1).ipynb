{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zhTS7qKqYta",
        "outputId": "2de56a96-a991-404f-e4d7-6d24fc8f8650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (optional: skip if Colab already has them)\n",
        "!pip install tensorflow numpy pandas nltk matplotlib\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"ğŸ“‚ Upload your text dataset (e.g. simpsons.txt)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get uploaded file name dynamically\n",
        "file_name = list(uploaded.keys())[0]\n",
        "text_path = file_name\n",
        "\n",
        "with open(text_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"âœ… File loaded:\", file_name)\n",
        "print(\"Characters in corpus:\", len(raw_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "8X2BF-7nqh9X",
        "outputId": "4606d7ef-0e1d-417b-b047-991968194821"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Upload your text dataset (e.g. simpsons.txt)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-06644358-147f-4c1d-9adf-57cbb49c7086\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-06644358-147f-4c1d-9adf-57cbb49c7086\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving simpsons_style_50000.txt to simpsons_style_50000.txt\n",
            "âœ… File loaded: simpsons_style_50000.txt\n",
            "Characters in corpus: 3165116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.replace('\\r', ' ')\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "text = clean_text(raw_text)\n",
        "\n",
        "# Build tokenizer\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([text])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "\n",
        "# Create sequences\n",
        "SEQ_LENGTH = 20\n",
        "tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "sequences = []\n",
        "for i in range(SEQ_LENGTH, len(tokens)):\n",
        "    seq = tokens[i-SEQ_LENGTH:i+1]\n",
        "    sequences.append(seq)\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "# Split inputs and targets\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC12XvwJqmgJ",
        "outputId": "c71f7ed6-2d7e-40eb-d095-039c628ca251"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 193\n",
            "X shape: (508871, 20) y shape: (508871, 193)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 128\n",
        "LSTM_UNITS = 256\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 3\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=SEQ_LENGTH),\n",
        "    LSTM(LSTM_UNITS, return_sequences=False),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Optional: save checkpoints\n",
        "os.makedirs('models', exist_ok=True)\n",
        "checkpoint_path = 'models/script_gen_epoch_{epoch:02d}.h5'\n",
        "cp = ModelCheckpoint(checkpoint_path, save_weights_only=False)\n",
        "\n",
        "history = model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[cp])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "T8LbnURLqs9I",
        "outputId": "96d9b133-1bf7-43fe-ddd4-d5ee065bba71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m3976/3976\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.6125 - loss: 1.5485"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3976/3976\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 183ms/step - accuracy: 0.6125 - loss: 1.5484\n",
            "Epoch 2/3\n",
            "\u001b[1m3976/3976\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.7116 - loss: 0.9082"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3976/3976\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m737s\u001b[0m 182ms/step - accuracy: 0.7116 - loss: 0.9082\n",
            "Epoch 3/3\n",
            "\u001b[1m3976/3976\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.7121 - loss: 0.9062"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3976/3976\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m743s\u001b[0m 182ms/step - accuracy: 0.7121 - loss: 0.9062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seed_text, gen_len=50, seq_length=SEQ_LENGTH):\n",
        "    result = []\n",
        "    in_text = seed_text\n",
        "    for _ in range(gen_len):\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        encoded = encoded[-seq_length:]\n",
        "        encoded = np.pad(encoded, (seq_length - len(encoded), 0), mode='constant')\n",
        "        yhat = model.predict(encoded.reshape(1, -1), verbose=0)\n",
        "        yhat_idx = np.argmax(yhat)\n",
        "        out_word = tokenizer.index_word.get(yhat_idx, '')\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)\n",
        "\n",
        "# Try generating a sample script\n",
        "seed = \"Homer:\"\n",
        "print(generate_text(model, tokenizer, seed, gen_len=40))\n"
      ],
      "metadata": {
        "id": "d8Frt7SSq1VB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3472336e-cc79-42d2-8c4f-744bf71cb19f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you canâ€™t prove it was meâ€¦ okay maybe you can sighs lisa you canâ€™t prove it was meâ€¦ okay maybe you can sighs lisa you canâ€™t prove it was meâ€¦ okay maybe you can sighs lisa you canâ€™t prove it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model\n",
        "model.save(\"tv_script_model.h5\")\n",
        "\n",
        "# Download trained model\n",
        "from google.colab import files\n",
        "files.download(\"tv_script_model.h5\")\n"
      ],
      "metadata": {
        "id": "lQWi993UrDYb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "cad71a42-729a-4ed2-a1e6-6c3afab4b18a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_591b9426-2017-410d-82bd-30d1350bcbd8\", \"tv_script_model.h5\", 5656148)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# ğŸ§  TV Script / Drama Model â€“ Testing\n",
        "# =====================================\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# -------------------------------------\n",
        "# ğŸ”¹ Step 1: Load Trained Model\n",
        "# -------------------------------------\n",
        "\n",
        "# If you saved your model after training:\n",
        "model = load_model(\"tv_script_model.h5\")\n",
        "print(\"âœ… Model loaded successfully!\")\n",
        "\n",
        "# -------------------------------------\n",
        "# ğŸ”¹ Step 2: Load or Rebuild Tokenizer\n",
        "# -------------------------------------\n",
        "\n",
        "# Option A: Load saved tokenizer (recommended)\n",
        "# with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "#     tokenizer = pickle.load(f)\n",
        "\n",
        "# Option B: Rebuild tokenizer from your dataset (if you didnâ€™t save it)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Use the file_name variable from the previous cell\n",
        "dataset_path = file_name\n",
        "with open(dataset_path, \"r\", encoding=\"utf-8\", errors='ignore') as f:\n",
        "    text = f.read()\n",
        "\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([text])\n",
        "print(\"âœ… Tokenizer ready! Vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# -------------------------------------\n",
        "# ğŸ”¹ Step 3: Text Generation (Simple)\n",
        "# -------------------------------------\n",
        "\n",
        "def generate_text(model, tokenizer, seed_text, gen_len=50, seq_length=20):\n",
        "    result = []\n",
        "    in_text = seed_text\n",
        "    for _ in range(gen_len):\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        encoded = encoded[-seq_length:]\n",
        "        encoded = np.pad(encoded, (seq_length - len(encoded), 0), mode='constant')\n",
        "        encoded = encoded.reshape(1, -1)\n",
        "\n",
        "        yhat = model.predict(encoded, verbose=0)\n",
        "        yhat_idx = np.argmax(yhat)\n",
        "        out_word = tokenizer.index_word.get(yhat_idx, '')\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)\n",
        "\n",
        "# -------------------------------------\n",
        "# ğŸ”¹ Step 4: Test Generation\n",
        "# -------------------------------------\n",
        "\n",
        "print(\"ğŸ­ Example 1:\")\n",
        "print(generate_text(model, tokenizer, \"Sherlock:\", gen_len=40))\n",
        "\n",
        "print(\"\\nğŸ­ Example 2:\")\n",
        "print(generate_text(model, tokenizer, \"Rachel:\", gen_len=40))\n",
        "\n",
        "# -------------------------------------\n",
        "# ğŸ”¹ Step 5: (Optional) Creative Sampling\n",
        "# -------------------------------------\n",
        "\n",
        "def sample_with_temperature(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-10) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def generate_text_temp(model, tokenizer, seed_text, gen_len=50, seq_length=20, temperature=0.8):\n",
        "    result = []\n",
        "    in_text = seed_text\n",
        "    for _ in range(gen_len):\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        encoded = encoded[-seq_length:]\n",
        "        encoded = np.pad(encoded, (seq_length - len(encoded), 0), mode='constant')\n",
        "        yhat = model.predict(encoded.reshape(1, -1), verbose=0)[0]\n",
        "        yhat_idx = sample_with_temperature(yhat, temperature)\n",
        "        out_word = tokenizer.index_word.get(yhat_idx, '')\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)\n",
        "\n",
        "# Example creative generation\n",
        "print(\"\\nğŸ­ Creative Example (temperature=0.7):\")\n",
        "print(generate_text_temp(model, tokenizer, \"Chandler:\", gen_len=60, temperature=0.7))"
      ],
      "metadata": {
        "id": "u7au_Lj42-2O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe526bf6-b40c-4986-a2aa-ad28ae57cca0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model loaded successfully!\n",
            "âœ… Tokenizer ready! Vocabulary size: 193\n",
            "ğŸ­ Example 1:\n",
            "this donut tastes like bad decisions sarcastically lisa you canâ€™t prove it was meâ€¦ okay maybe you can sighs lisa you canâ€™t prove it was meâ€¦ okay maybe you can sighs lisa you canâ€™t prove it was meâ€¦ okay maybe\n",
            "\n",
            "ğŸ­ Example 2:\n",
            "this donut tastes like bad decisions sarcastically lisa you canâ€™t prove it was meâ€¦ okay maybe you can sighs lisa you canâ€™t prove it was meâ€¦ okay maybe you can sighs lisa you canâ€™t prove it was meâ€¦ okay maybe\n",
            "\n",
            "ğŸ­ Creative Example (temperature=0.7):\n",
            "nothing like mild panic you had one job and somehow you still outdid yourself sarcastically brandine oh great another meeting that couldâ€™ve been an email sarcastically professor frink i canâ€™t believe this happened again deadpan nelson you canâ€™t prove it was meâ€¦ okay maybe you can whispering principal skinner you had one job and somehow you still outdid yourself sarcastically\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# ğŸ”¹ Characters list (you can edit this)\n",
        "characters = [\n",
        "    \"Homer\", \"Marge\", \"Bart\", \"Lisa\", \"Maggie\", \"Mr. Burns\", \"Smithers\",\n",
        "    \"Moe\", \"Barney\", \"Ned\", \"Milhouse\", \"Chief Wiggum\", \"Krusty\",\n",
        "    \"Apu\", \"Principal Skinner\", \"Ralph\", \"Comic Book Guy\", \"Lenny\", \"Carl\"\n",
        "]\n",
        "\n",
        "# ğŸ”¹ Function to generate one dialogue line\n",
        "def generate_line(model, tokenizer, seed_text, gen_len=50, seq_length=20, temperature=0.8):\n",
        "    in_text = seed_text\n",
        "    result = []\n",
        "    for _ in range(gen_len):\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        encoded = encoded[-seq_length:]\n",
        "        encoded = np.pad(encoded, (seq_length - len(encoded), 0), mode='constant')\n",
        "        yhat = model.predict(encoded.reshape(1, -1), verbose=0)[0]\n",
        "\n",
        "        # Temperature sampling for natural variety\n",
        "        # Add a small epsilon to prevent log(0)\n",
        "        preds = np.asarray(yhat).astype('float64') + 1e-10\n",
        "        preds = np.log(preds) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        # Ensure probabilities sum to 1 after exponentiation\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        yhat_idx = np.argmax(probas)\n",
        "\n",
        "        out_word = tokenizer.index_word.get(yhat_idx, '')\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)\n",
        "\n",
        "# ğŸ”¹ Function to generate a 10-line dialogue\n",
        "def generate_dialogue(model, tokenizer, characters, n_lines=10):\n",
        "    print(\"ğŸ¬ Generated TV Scene:\\n\")\n",
        "    for i in range(n_lines):\n",
        "        char = random.choice(characters)\n",
        "        line = generate_line(model, tokenizer, seed_text=char + \":\", gen_len=random.randint(20, 50))\n",
        "        print(f\"{char}: {line.strip().capitalize()}\")\n",
        "    print(\"\\nâœ… 10-line script generated successfully!\")\n",
        "\n",
        "# ğŸ”¹ Run the dialogue generator\n",
        "generate_dialogue(model, tokenizer, characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GJnQLejaKo8",
        "outputId": "aab31cf5-f31a-47ef-d925-e095c8c3e003"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¬ Generated TV Scene:\n",
            "\n",
            "Apu: Thatâ€™s not how gravity works but go on shouting lenny oh great another meeting that couldâ€™ve been an email shouting maude weâ€™ve officially reached peak nonsense mayor quimby weâ€™ve officially reached peak nonsense shouting principal skinner\n",
            "Ralph: Thatâ€™s not a bug itâ€™s a feature sarcastically sideshow bob d'oh not again shouting hans moleman you call that a plan i call that chaos deadpan smithers thatâ€™s not how gravity works but go on muttering ralph i canâ€™t believe this happened again\n",
            "Smithers: Why is it always me sarcastically agnes skinner this donut tastes like bad decisions whispering eddie oh great another meeting that couldâ€™ve been an email shouting otto you canâ€™t prove it was meâ€¦ okay maybe you can excitedly lionel hutz thatâ€™s not how gravity works but go on muttering\n",
            "Moe: Oh great another meeting that couldâ€™ve been an email excitedly moe i was told there would be snacks deadpan krusty i could really use a nap right now deadpan\n",
            "Carl: Whoever said laughter is the best medicine never met my cooking whispering helen lovejoy thatâ€™s not how gravity works but go on muttering\n",
            "Krusty: This donut tastes like bad decisions excitedly brandine thereâ€™s no problem that canâ€™t be made worse by my advice muttering troy mcclure thereâ€™s no problem\n",
            "Lisa: Weâ€™ve officially reached peak nonsense sighs cletus whoever said laughter is the best medicine never met my cooking shouting snake thatâ€™s not a bug itâ€™s a feature\n",
            "Apu: We tried being normal once it was overrated muttering lisa you canâ€™t prove it was meâ€¦ okay maybe you can sarcastically lionel hutz i was told there would be snacks laughing agnes skinner oh great\n",
            "Marge: Thereâ€™s no problem that canâ€™t be made worse by my advice muttering principal skinner if this is a test iâ€™m\n",
            "Principal Skinner: Thereâ€™s nothing like mild panic to start the day deadpan lou you had one job and somehow you still outdid yourself sighs reverend lovejoy thatâ€™s\n",
            "\n",
            "âœ… 10-line script generated successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: Install + Upload your 2 files\n",
        "!pip install gradio tensorflow -q\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"Please upload these two files: 'tv_script_model.h5' and 'simpsons_style_50000.txt'\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Identify the uploaded files\n",
        "model_file_uploaded = None\n",
        "dataset_file_uploaded = None\n",
        "\n",
        "for uploaded_name in uploaded.keys():\n",
        "    if \"tv_script_model\" in uploaded_name and uploaded_name.endswith(\".h5\"):\n",
        "        model_file_uploaded = uploaded_name\n",
        "    elif \"simpsons_style_50000\" in uploaded_name and uploaded_name.endswith(\".txt\"):\n",
        "        dataset_file_uploaded = uploaded_name\n",
        "\n",
        "if not model_file_uploaded:\n",
        "    raise FileNotFoundError(\"tv_script_model.h5 not found among uploaded files!\")\n",
        "if not dataset_file_uploaded:\n",
        "    raise FileNotFoundError(\"simpsons_style_50000.txt not found among uploaded files!\")\n",
        "\n",
        "# Rename files to their expected names if necessary\n",
        "if model_file_uploaded != \"tv_script_model.h5\":\n",
        "    if os.path.exists(\"tv_script_model.h5\"):\n",
        "        os.remove(\"tv_script_model.h5\") # Remove old file if it exists, to avoid error during rename\n",
        "    os.rename(model_file_uploaded, \"tv_script_model.h5\")\n",
        "    print(f\"Renamed '{model_file_uploaded}' to 'tv_script_model.h5'\")\n",
        "    model_file_uploaded = \"tv_script_model.h5\" # Update name for consistency\n",
        "\n",
        "if dataset_file_uploaded != \"simpsons_style_50000.txt\":\n",
        "    if os.path.exists(\"simpsons_style_50000.txt\"):\n",
        "        os.remove(\"simpsons_style_50000.txt\") # Remove old file if it exists\n",
        "    os.rename(dataset_file_uploaded, \"simpsons_style_50000.txt\")\n",
        "    print(f\"Renamed '{dataset_file_uploaded}' to 'simpsons_style_50000.txt'\")\n",
        "    dataset_file_uploaded = \"simpsons_style_50000.txt\" # Update name for consistency\n",
        "\n",
        "print(\"Files uploaded and renamed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "4eegw3bdmXtj",
        "outputId": "795b923f-4f7f-4727-fd0f-e9c3130d6ac1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload these two files: 'tv_script_model.h5' and 'simpsons_style_50000.txt'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bdf0318a-81a7-4126-9c17-114563447779\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bdf0318a-81a7-4126-9c17-114563447779\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving simpsons_style_50000.txt to simpsons_style_50000 (1).txt\n",
            "Saving tv_script_model.h5 to tv_script_model (1).h5\n",
            "Renamed 'tv_script_model (1).h5' to 'tv_script_model.h5'\n",
            "Renamed 'simpsons_style_50000 (1).txt' to 'simpsons_style_50000.txt'\n",
            "Files uploaded and renamed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8c85505",
        "outputId": "e07bedf7-8bd4-47f4-b0da-ef970443972f"
      },
      "source": [
        "import os\n",
        "\n",
        "# Check if the file 'tv_script_model (2).h5' exists before attempting to rename it\n",
        "if os.path.exists('tv_script_model (2).h5'):\n",
        "    os.rename('tv_script_model (2).h5', 'tv_script_model.h5')\n",
        "    print(\"Renamed 'tv_script_model (2).h5' to 'tv_script_model.h5'\")\n",
        "else:\n",
        "    print(\"File 'tv_script_model (2).h5' not found. Please ensure it was uploaded.\")\n",
        "\n",
        "# Ensure the dataset file is also correctly named if it was uploaded with an altered name\n",
        "if os.path.exists('simpsons_style_50000 (1).txt'):\n",
        "    os.rename('simpsons_style_50000 (1).txt', 'simpsons_style_50000.txt')\n",
        "    print(\"Renamed 'simpsons_style_50000 (1).txt' to 'simpsons_style_50000.txt'\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'tv_script_model (2).h5' not found. Please ensure it was uploaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Load model + REBUILD tokenizer from the text file\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Load the model\n",
        "model = load_model(\"tv_script_model.h5\")\n",
        "print(\"Model loaded\")\n",
        "\n",
        "# Rebuild the exact same tokenizer you used during training\n",
        "with open(\"simpsons_style_50000.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "print(f\"Tokenizer rebuilt! Vocabulary size: {len(tokenizer.word_index) + 1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAOq_Y5NnaV_",
        "outputId": "72d095e1-b7ee-43ec-bee4-2bfab4dab072"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded\n",
            "Tokenizer rebuilt! Vocabulary size: 193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# CELL 3 â€” NEW FAST GRADIO UI\n",
        "# ============================\n",
        "\n",
        "import gradio as gr\n",
        "import random\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "SEQ_LENGTH = 20\n",
        "\n",
        "# -------------------------------\n",
        "# âš¡ FAST Temperature Sampling\n",
        "# -------------------------------\n",
        "def sample_with_temp(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds + 1e-10) / temperature\n",
        "    preds = np.exp(preds)\n",
        "    preds = preds / np.sum(preds)\n",
        "    return np.argmax(np.random.multinomial(1, preds, 1))\n",
        "\n",
        "# -------------------------------\n",
        "# âš¡ SUPER FAST SINGLE-LINE GENERATOR\n",
        "# -------------------------------\n",
        "def generate_one_line_fast(model, tokenizer, speaker, temperature=0.95, max_words=25):\n",
        "    text = f\"{speaker}:\"\n",
        "\n",
        "    for _ in range(max_words):\n",
        "        seq = tokenizer.texts_to_sequences([text])[0]\n",
        "        seq = seq[-SEQ_LENGTH:]\n",
        "\n",
        "        padded = pad_sequences([seq], maxlen=SEQ_LENGTH)\n",
        "        pred = model.predict_on_batch(padded)[0]\n",
        "\n",
        "        next_idx = sample_with_temp(pred, temperature)\n",
        "        word = tokenizer.index_word.get(next_idx, \"\")\n",
        "\n",
        "        if not word:\n",
        "            break\n",
        "\n",
        "        text += \" \" + word\n",
        "\n",
        "        if word in [\".\", \"!\", \"?\"]:\n",
        "            break\n",
        "\n",
        "    # Clean final text\n",
        "    line = text.split(\":\", 1)[1].strip()\n",
        "    if not line:\n",
        "        return \"*Says nothing... awkward silence*\"\n",
        "\n",
        "    return line[0].upper() + line[1:] + random.choice([\".\", \"!\", \"?\", \"...\"])\n",
        "\n",
        "# -------------------------------\n",
        "# âš¡ BULK GENERATOR (FAST & SAFE)\n",
        "# -------------------------------\n",
        "def fast_generate_script(characters_str, num_lines, temperature, first_speaker):\n",
        "    chars = [c.strip() for c in characters_str.split(\",\") if c.strip()]\n",
        "    if not chars:\n",
        "        return \"âŒ Error: Please enter at least one character.\"\n",
        "\n",
        "    # Who talks first?\n",
        "    if first_speaker != \"Random\" and first_speaker in chars:\n",
        "        start = chars.index(first_speaker)\n",
        "    else:\n",
        "        start = random.randint(0, len(chars) - 1)\n",
        "\n",
        "    # Speaker rotation order\n",
        "    order = [chars[(start + i) % len(chars)] for i in range(num_lines)]\n",
        "\n",
        "    lines = []\n",
        "    for sp in order:\n",
        "        line = generate_one_line_fast(model, tokenizer, sp, temperature)\n",
        "        lines.append(f\"**{sp}:** {line}\")\n",
        "\n",
        "    # Friendly preview limit\n",
        "    preview = \"\\n\\n\".join(lines[:300])\n",
        "    if num_lines > 300:\n",
        "        preview += \"\\n\\n---\\nâš ï¸ *Preview limited to first 300 lines for performance.*\"\n",
        "\n",
        "    return preview\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# ğŸŒŸ GRADIO UI â€” CLEAN, FRIENDLY, EASY FOR USERS\n",
        "# -----------------------------------------------------------\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Simpsons Script Generator\") as app:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ğŸ© **Simpsons Script Generator**\n",
        "    Ultra-fast, error-proof, and fun!\n",
        "    Generate your own Simpsons-style dialogue powered by your LSTM model.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "\n",
        "            chars = gr.Textbox(\n",
        "                label=\"Characters (comma-separated)\",\n",
        "                value=\"Homer, Marge, Bart, Lisa, Mr. Burns, Moe\",\n",
        "                placeholder=\"Example: Homer, Bart, Moe, Lisa\"\n",
        "            )\n",
        "\n",
        "            lines_slider = gr.Slider(\n",
        "                10, 2000, 200, step=10,\n",
        "                label=\"Number of Dialogue Lines\"\n",
        "            )\n",
        "\n",
        "            temp = gr.Slider(\n",
        "                0.6, 1.4, 0.95, step=0.05,\n",
        "                label=\"Creativity Level (Temperature)\"\n",
        "            )\n",
        "\n",
        "            first = gr.Dropdown(\n",
        "                [\"Random\", \"Homer\", \"Marge\", \"Bart\", \"Lisa\", \"Mr. Burns\", \"Moe\", \"Krusty\"],\n",
        "                label=\"Who Speaks First?\",\n",
        "                value=\"Random\"\n",
        "            )\n",
        "\n",
        "            btn = gr.Button(\"ğŸ¬ Generate Script\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            output = gr.Markdown()\n",
        "\n",
        "    btn.click(\n",
        "        fast_generate_script,\n",
        "        inputs=[chars, lines_slider, temp, first],\n",
        "        outputs=[output]\n",
        "    )\n",
        "\n",
        "app.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "0ld48yRY1dtq",
        "outputId": "4d56f1e8-95db-4452-ee7f-375c6e257b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6b997f1b66e59d73f1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6b997f1b66e59d73f1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}